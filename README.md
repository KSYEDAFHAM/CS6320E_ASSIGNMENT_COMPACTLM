# Compact Language Model Challenge - Telugu & Marathi

**Course:** CS6320E - Topics in Natural Language Processing  
**Task:** Next-token prediction (Causal Language Modeling)  
**Languages:** Telugu, Marathi

---

## ğŸ“‹ Table of Contents
- [Project Overview](#project-overview)
- [Team Information](#team-information)
- [Model Architecture](#model-architecture)
- [Setup Instructions](#setup-instructions)
- [Dataset](#dataset)
- [Training](#training)
- [Inference](#inference)
- [Performance Metrics](#performance-metrics)
- [Results](#results)
- [Evaluation Checklist](#evaluation-checklist)

---

## ğŸ¯ Project Overview

This project implements compact, sample-efficient language models for Telugu and Marathi using fine-tuned GPT-2 architecture. The models perform next-token prediction on low-resource Indian languages.

**Key Features:**
- GPT-2 based causal language model
- Early stopping mechanism for optimal performance
- Separate models for Telugu and Marathi
- Gradient checkpointing for memory efficiency
- Mixed precision training (FP16)

---

## ğŸ‘¥ Team Information

**Team Name:** [Your Team Name]  
**Members:**
- [Member 1 Name] - [Roll Number]
- [Member 2 Name] - [Roll Number] (if applicable)

---

## ğŸ—ï¸ Model Architecture

### Base Model
- **Architecture:** GPT-2 (Transformer-based decoder)
- **Model Type:** Autoregressive Causal Language Model
- **Base Parameters:** ~124M (GPT-2 base)
- **Tokenizer:** GPT-2 tokenizer with custom tokens (<SEP>, <PAD>)

### Model Configuration
- **Hidden Size:** 768
- **Number of Layers:** 12
- **Attention Heads:** 12
- **Vocabulary Size:** Extended with Telugu/Marathi tokens
- **Maximum Sequence Length:** 256 tokens
- **Total Trainable Parameters:** ~124M + embeddings

### Custom Modifications
- Added special tokens: `<SEP>` (separator), `<PAD>` (padding)
- Resized token embeddings to accommodate new tokens
- Configured pad_token_id for proper masking

---

## ğŸ› ï¸ Setup Instructions

### Prerequisites
- Python 3.8+
- CUDA-capable GPU (recommended)
- 16GB+ RAM
- 10GB+ disk space

### Installation

1. **Clone the repository:**
```bash
git clone [your-repo-url]
cd [repo-name]
```

2. **Create virtual environment:**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies:**
```bash
pip install -r requirements.txt
```

### Required Libraries
```txt
torch>=2.0.0
transformers>=4.30.0
datasets>=2.12.0
numpy>=1.24.0
scikit-learn>=1.2.0
pandas>=2.0.0
tqdm>=4.65.0
evaluate>=0.4.0
rouge-score>=0.1.2
bert-score>=0.3.13
nltk>=3.8.0
```

---

## ğŸ“Š Dataset

### Dataset Structure
The dataset is provided in JSONL format with the following fields:
- **input:** Source text in Telugu/Marathi
- **target:** Target text for next-token prediction

### Example:
```json
{
  "row": {
    "input": "à°«à°¿à°¬à±à°°à°µà°°à°¿ 23, 2019 174 à°à°¨à±à°¨à±‹ à°­à°¾à°°à±€ à°…à°‚à°šà°¨à°¾à°²à°¤à±‹ à°µà°šà±à°šà°¿à°¨ à°®à°¹à°¾à°¨à±à°­à°¾à°µà±à°¡à± à°à°¨à± . à°Ÿà°¿ . à°†à°°à± à°¬à°¯à±‹à°ªà°¿à°•à± à°°à°‚à°¡à± à°ªà°¾à°°à±à°Ÿà±à°²à± à°¨à°¿à°°à°¾à°¶à°ªà°°à°šà°¾à°¯à°¿ .",
    "target": "à°®à°¹à°¾à°¨à°¾à°¯à°•à±à°¡à± à°®à°‚à°šà±‡à°¶à°¾à°¡à±à°—à°¾"
  }
}
```

### Dataset Location
Download from: [Google Drive Link](https://drive.google.com/drive/folders/1d2-Uf2yuCoJ2QLDd03pC-WyBR55JSE0-)

**Expected files:**
- `train_te.jsonl` - Telugu training data
- `validation_te.jsonl` - Telugu validation data
- `train_mr.jsonl` - Marathi training data
- `validation_mr.jsonl` - Marathi validation data

### Preprocessing
The training script automatically:
1. Cleans whitespace and normalizes text
2. Merges input and target with `<SEP>` separator
3. Tokenizes and truncates to max_length (256 tokens)
4. Creates attention masks and labels

---

## ğŸš€ Training

### Training Script
Use `train_transformer_telugu_gpt2.py` for training.

### Training Telugu Model
```bash
python train_transformer_telugu_gpt2.py \
  --train_path ./data/train_te.jsonl \
  --valid_path ./data/validation_te.jsonl \
  --output_dir ./models/gpt2_telugu \
  --epochs 30 \
  --batch_size 16 \
  --lr 5e-5 \
  --patience 3
```

### Training Marathi Model
```bash
python train_transformer_telugu_gpt2.py \
  --train_path ./data/train_mr.jsonl \
  --valid_path ./data/validation_mr.jsonl \
  --output_dir ./models/gpt2_marathi \
  --epochs 30 \
  --batch_size 16 \
  --lr 5e-5 \
  --patience 3
```

### Hyperparameters
| Parameter | Value | Description |
|-----------|-------|-------------|
| Learning Rate | 5e-5 | Initial learning rate |
| Batch Size | 16 | Per-device batch size |
| Gradient Accumulation | 2 | Effective batch size: 32 |
| Epochs | 30 | Maximum training epochs |
| Warmup Ratio | 0.1 | Learning rate warmup |
| Early Stopping Patience | 3 | Stop after 3 epochs without improvement |
| Max Sequence Length | 256 | Maximum tokens per sequence |
| Optimizer | AdamW | Default from Trainer |
| LR Scheduler | Cosine | Cosine annealing |
| Max Grad Norm | 1.0 | Gradient clipping |
| FP16 | True | Mixed precision training |
| Gradient Checkpointing | True | Memory optimization |

### Training Strategy
1. **Warmup Phase:** 10% of total steps with linear warmup
2. **Training Phase:** Cosine learning rate decay
3. **Early Stopping:** Monitor validation loss, patience=3
4. **Checkpointing:** Save best model based on eval_loss
5. **Evaluation:** Every epoch

### Hardware Used
- **GPU:** [Specify your GPU, e.g., NVIDIA A100 40GB]
- **Training Time:** [~X hours for Telugu, ~Y hours for Marathi]
- **Memory Usage:** [Peak GPU memory]

### Random Seeds
- **Training seed:** Set via `PYTHONHASHSEED`, `torch.manual_seed()`, etc.
- **Reproducibility:** Use same seed for consistent results

---

## ğŸ”® Inference

### Running Inference
```bash
python infer.py \
  --model_path ./models/gpt2_telugu \
  --test_file ./data/test_te.jsonl \
  --output_file ./results/predictions_te.jsonl \
  --batch_size 32 \
  --device cuda:0
```

### Inference Script Usage
```python
# Example usage
from infer import load_model, predict_next_token

# Load model
model, tokenizer = load_model("./models/gpt2_telugu")

# Predict
input_text = "à°«à°¿à°¬à±à°°à°µà°°à°¿ 23, 2019"
probabilities = predict_next_token(model, tokenizer, input_text)
```

### Expected Output Format
- Next-token probability distribution
- Top-k predictions with probabilities
- Perplexity score per sample

---

## ğŸ“ˆ Performance Metrics

### Primary Metric
- **Perplexity:** Lower is better (exponential of cross-entropy loss)

### Required Metrics

#### 1. Model Quality Metrics
```bash
python measure_performance.py --model_path ./models/gpt2_telugu --test_file ./data/test_te.jsonl
```

**Computed Metrics:**
- âœ… Perplexity
- âœ… Cross-entropy / Bits-per-token
- âœ… Token-level top-1 accuracy
- âœ… ROUGE scores (ROUGE-1, ROUGE-2, ROUGE-L)
- âœ… BLEU Score
- âœ… BERTScore

#### 2. Efficiency Metrics
- **Parameter Count:** Total and trainable parameters
- **Inference Latency:** Median and P95 time per token (ms)
- **Throughput:** Tokens per second
- **FLOPs/MACs:** Estimated operations
- **Model Size:** File size (MB) and quantized size
- **Memory Usage:** Peak GPU/CPU memory during inference

### Metric Calculation Methods
- **Perplexity:** `exp(cross_entropy_loss)`
- **Parameters:** `sum(p.numel() for p in model.parameters())`
- **Latency:** Measured using `time.perf_counter()` over 100 runs
- **Throughput:** `num_tokens / total_time`
- **FLOPs:** Estimated using PyTorch profiler or `thop` library
- **Memory:** Tracked using `torch.cuda.max_memory_allocated()`

---

## ğŸ“Š Results

### Telugu Model Performance

| Metric | Value |
|--------|-------|
| Test Perplexity | [X.XX] |
| Cross-Entropy Loss | [X.XX] |
| Top-1 Accuracy | [XX.X%] |
| ROUGE-1 | [X.XX] |
| ROUGE-L | [X.XX] |
| BLEU | [X.XX] |
| BERTScore F1 | [X.XX] |

### Marathi Model Performance

| Metric | Value |
|--------|-------|
| Test Perplexity | [X.XX] |
| Cross-Entropy Loss | [X.XX] |
| Top-1 Accuracy | [XX.X%] |
| ROUGE-1 | [X.XX] |
| ROUGE-L | [X.XX] |
| BLEU | [X.XX] |
| BERTScore F1 | [X.XX] |

### Efficiency Metrics

| Metric | Telugu | Marathi |
|--------|--------|---------|
| Total Parameters | [XXX M] | [XXX M] |
| Trainable Parameters | [XXX M] | [XXX M] |
| Model Size (MB) | [XXX] | [XXX] |
| Inference Latency (median, ms) | [X.XX] | [X.XX] |
| Inference Latency (p95, ms) | [X.XX] | [X.XX] |
| Throughput (tokens/sec) | [XXX] | [XXX] |
| FLOPs per token | [XXX M] | [XXX M] |
| Peak Memory (GB) | [X.XX] | [X.XX] |

### Sample Outputs

#### Telugu Example
```
Input: "à°«à°¿à°¬à±à°°à°µà°°à°¿ 23, 2019 174 à°à°¨à±à°¨à±‹ à°­à°¾à°°à±€ à°…à°‚à°šà°¨à°¾à°²à°¤à±‹ à°µà°šà±à°šà°¿à°¨"
Predicted: "à°®à°¹à°¾à°¨à±à°­à°¾à°µà±à°¡à±"
Confidence: 0.85
```

#### Marathi Example
```
Input: "[Marathi input text]"
Predicted: "[Predicted token]"
Confidence: 0.XX
```

---

## âœ… Evaluation Checklist

### Before Evaluation Session

#### âœ… Technical Preparation
- [ ] Laptop with fully charged battery and charger
- [ ] Complete project code ready to run
- [ ] All environments and dependencies installed locally
- [ ] Models saved separately for Telugu and Marathi
- [ ] Internet independent - all resources available locally
- [ ] Test inference pipeline end-to-end

#### âœ… Model Files
- [ ] Telugu model checkpoint saved
- [ ] Marathi model checkpoint saved
- [ ] Tokenizer files for both languages
- [ ] Configuration files (config.json)

#### âœ… Scripts Ready
- [ ] `infer.py` - Real-time inference on test sets
- [ ] `measure_performance.py` - Compute all metrics
- [ ] Scripts accept command-line arguments
- [ ] File paths easily adjustable

#### âœ… Metrics Scripts
- [ ] Perplexity calculation
- [ ] ROUGE scores (ROUGE-1, ROUGE-2, ROUGE-L)
- [ ] BLEU Score
- [ ] METEOR (if applicable)
- [ ] BERTScore (if applicable)
- [ ] Clear display of numerical values
- [ ] Documentation of preprocessing steps

#### âœ… Documentation
- [ ] README.md with clear instructions
- [ ] Report.pdf (4-6 pages)
- [ ] requirements.txt or environment.yml
- [ ] submission_metadata.json
- [ ] Training logs and hyperparameters recorded

### During Evaluation

#### Test Set Evaluation
- [ ] Load provided unseen test set(s)
- [ ] Run model in real-time
- [ ] Generate outputs without code modification
- [ ] Display metrics clearly

### Viva Preparation Topics

#### 1. Model Architecture
- [ ] Explain GPT-2 architecture choice
- [ ] Input/output format justification
- [ ] Tokenization approach (BPE)
- [ ] Embedding technique
- [ ] Hidden layer size: 768
- [ ] Number of layers: 12
- [ ] Total trainable parameters: ~124M

#### 2. Data Handling
- [ ] Dataset sources explained
- [ ] Preprocessing steps documented
- [ ] Text cleaning procedures
- [ ] Data augmentation (if any)
- [ ] Normalization techniques

#### 3. Experimental Setup
- [ ] Training strategy explained
- [ ] Hyperparameters justified:
  - Learning rate: 5e-5
  - Batch size: 16 (effective 32)
  - Dropout: default
  - Optimizer: AdamW
- [ ] Number of epochs: 30 (with early stopping)
- [ ] Early stopping implementation (patience=3)
- [ ] Checkpointing strategy

#### 4. Model Output Analysis
- [ ] Sample outputs prepared
- [ ] Error analysis ready
- [ ] Strengths identified
- [ ] Weaknesses understood
- [ ] Improvement suggestions

---

## ğŸ“ Project Structure
```
project/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ gpt2_telugu/
â”‚   â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”‚   â”œâ”€â”€ config.json
â”‚   â”‚   â””â”€â”€ tokenizer files
â”‚   â””â”€â”€ gpt2_marathi/
â”‚       â”œâ”€â”€ pytorch_model.bin
â”‚       â”œâ”€â”€ config.json
â”‚       â””â”€â”€ tokenizer files
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train_te.jsonl
â”‚   â”œâ”€â”€ validation_te.jsonl
â”‚   â”œâ”€â”€ train_mr.jsonl
â”‚   â””â”€â”€ validation_mr.jsonl
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ predictions_te.jsonl
â”‚   â””â”€â”€ predictions_mr.jsonl
â”œâ”€â”€ train_transformer_telugu_gpt2.py
â”œâ”€â”€ infer.py
â”œâ”€â”€ measure_performance.py
â”œâ”€â”€ README.md
â”œâ”€â”€ report.pdf
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ submission_metadata.json
â””â”€â”€ environment.yml
```

---

## ğŸ” External Resources Declaration

All external resources used in this project:

### Pretrained Models
- **GPT-2 base:** Hugging Face (`gpt2`)
- **License:** MIT License
- **Source:** https://huggingface.co/gpt2

### Libraries & Tools
- **PyTorch:** BSD License
- **Transformers:** Apache 2.0
- **Datasets:** Apache 2.0
- **Standard libraries:** NumPy, Pandas (BSD licenses)

### Datasets
- **Training data:** Provided by course instructors
- **No additional labeled data used**

---

## ğŸ“„ Submission Metadata
```json
{
  "team_name": "[Your Team Name]",
  "members": [
    {
      "name": "[Member 1]",
      "roll_number": "[Roll 1]"
    },
    {
      "name": "[Member 2]",
      "roll_number": "[Roll 2]"
    }
  ],
  "external_resources": {
    "pretrained_models": ["gpt2"],
    "libraries": ["transformers", "torch", "datasets"],
    "hardware": "[GPU type]",
    "compute_time": "[X hours]"
  },
  "random_seeds": {
    "training": 42,
    "evaluation": 42
  },
  "date_submitted": "2025-11-03"
}
```

---

## ğŸ¤ Collaboration & Academic Integrity

- All code is original to our team
- High-level discussions with other students documented
- No code copying from other teams
- All external resources properly attributed
- No test data leakage into training/validation

---

## ğŸ“ License & Ethical Considerations

### Model License
This model is released for educational purposes only.

### Limitations
- Trained on limited data - may not generalize well
- Potential biases from training data
- Not suitable for production use without further testing
- Language-specific limitations for Telugu and Marathi

### Potential Biases
- Dataset bias toward specific domains
- Underrepresentation of certain dialects
- Historical biases in source text

---

## ğŸ“ Contact

For questions or issues:
- **Team:** [Your Team Name]
- **Email:** [your.email@example.com]
- **Course:** CS6320E - NLP

---

## ğŸ™ Acknowledgments

- Course instructors and TAs
- Hugging Face for transformers library
- OpenAI for GPT-2 architecture
- Dataset providers

---

**Last Updated:** [Date]  
**Submission Deadline:** November 3, 2025
